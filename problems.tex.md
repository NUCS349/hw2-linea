## Problems

## Code implementation (5 points)
Pass test cases by implementing the functions in the `code` directory.

Your grade for this section is defined by the autograder. If it says you got an 80/100,
you get 4 points here.

## Free response questions (5 points)

Answer the following free response questions in a separate document, 
saved as a .pdf and **uploaded to Canvas**.

#### 1. Perform regression (1 point) 
First, run experiments using the `PolynomialRegression` code you wrote.

Generate data (via the function you implemented `generate_regression_data`). Generate data from polynomial functions of the following degrees: {1,2,4,8}. Be sure to add a little bit of noise to each dataset you create. 

For each of your 4 generated datasets:
   1. Run your implementation of `PolynomialRegression` 10 times, one time for each degree from 1:10. 
   2. Create one plot for that dataset that shows mean squared error (vertical) of your regression as a function of degree (horizontal).
   
Now, pick a single data file (I suggest the one generated from a polynomial of degree 4, but it's your choice) to illustrate bias vs variance and the dangers of overfitting.
   1. Show a scatter plot of the dataset. Indicate the degree of the polynomial used to generate it in the title of the plot.
   2. Plot 3 functions learned with`PolynomialRegression` over the scatter plot: A functioned learned by a lower degree (e.g. if the data was generated with degree 4, then show a learned function with, say... degree 1); a function learned by the same degree (e.g. data generated with degree 4 and learned with a degree 4 polynomial) and the function that showed the lowest error learned with a higer degree polynomial than was used to generate the data.

#### 2. (1 point) In the plots of mean squared error vs degree you did for question 1, describe the relationship of the error to the degree of poynomial used to generate the data originally and the degree of polynomial used to fit the data. Is it always a good idea to use the highest degree polynomial to fit the data? Why or why not? 

#### 3. (0.5 points) Explain how to do classification via regression. Be clear. Use a graph to illustrate how it works. This graph doesn't need to be generated by your code, but it must be clearly labeled and made by you (not cut and pasted from somewhere).

#### 4. (0.5 points) Explain one weakness of classification via regression. Be clear. Use a graph to illustrate your point. This graph doesn't need to be generated by your code, but it must be clearly labeled and made by you (not cut and pasted from somewhere).

#### 5.(0.5 points) In the data directory, we provide 5 datasets as json files: blobs, circles, crossing, parallell_lines and transform_me. For each of these 5 provided datasets (mentioned above), generate a scatter plot of the data with your own code. Color each point colored according to its true class.  See load_json_data for a code example.  (Aside: you can upload these json files to http://ml-playground.com/ to see how different learning algorithms work on each dataset. This is not required for points, and you're definitely not allowed to submit scatter plots built by ml-playground, but it might be illuminating and useful for checking your work).

#### 6. (0.5 points) For each of these 5 datasets, plot the linear separator learned by your perceptron on top of the scatter plot. Note: Not all of the datasets will be linearly separable, so this line may not look great. If the perceptron never converged on a final line, pick the line it had generated by the time it reached max_iterations.

#### 7. (0.5 points) For each of the 5 datasets, say whether your perceptron successfuly separate the two classes on this dataset? Explain why or why not? 

#### 8. (0.5 points) In `code/perceptron.py`, you passed a test case by implementing the function `transform_data`. Describe the transformation you made to the input data to pass the test case. Explain how it allowed you to pass the test case (i.e. how did the change made to the data enable the system to do what it could not previously do?).


 


 
